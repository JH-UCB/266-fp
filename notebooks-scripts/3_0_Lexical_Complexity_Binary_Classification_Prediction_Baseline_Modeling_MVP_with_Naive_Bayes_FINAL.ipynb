{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@title Install Packages"
      ],
      "metadata": {
        "id": "xYRurcbg2fuq"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XWeMhKnlvi9F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2534e11-3572-41f3-c2d6-4aaae45982a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/491.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/116.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/183.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/143.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/194.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.3/118.3 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers\n",
        "!pip install -q torchinfo\n",
        "!pip install -q datasets\n",
        "!pip install -q evaluate\n",
        "!pip install -q nltk\n",
        "!pip install -q contractions\n",
        "!pip install -q hf_xet\n",
        "!pip install -q sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update\n",
        "! sudo apt-get install tree"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rX3VqZu11-s_",
        "outputId": "c90ba8bc-2d1f-4691-dc97-a55164588e5f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (185.1\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (185.1\r                                                                               \rGet:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,383 kB]\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:10 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,783 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,804 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,243 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,994 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,154 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,097 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,540 kB]\n",
            "Get:19 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,683 kB]\n",
            "Fetched 30.1 MB in 4s (6,871 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  tree\n",
            "0 upgraded, 1 newly installed, 0 to remove and 37 not upgraded.\n",
            "Need to get 47.9 kB of archives.\n",
            "After this operation, 116 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tree amd64 2.0.2-1 [47.9 kB]\n",
            "Fetched 47.9 kB in 1s (55.0 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package tree.\n",
            "(Reading database ... 126213 files and directories currently installed.)\n",
            "Preparing to unpack .../tree_2.0.2-1_amd64.deb ...\n",
            "Unpacking tree (2.0.2-1) ...\n",
            "Setting up tree (2.0.2-1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Imports\n",
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "import contractions\n",
        "\n",
        "import evaluate\n",
        "import transformers\n",
        "import torch\n",
        "\n",
        "from torchinfo import summary\n",
        "\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import sklearn\n",
        "\n",
        "import spacy\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, precision_recall_fscore_support, accuracy_score\n",
        "\n",
        "import sentencepiece"
      ],
      "metadata": {
        "id": "3wEgNBR6zosA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Mount Google Drive"
      ],
      "metadata": {
        "id": "rSP7bIn12YU7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWKPq7h01cXk",
        "outputId": "9ad1c9c3-73d0-4cf5-b1bd-e46c5161fb93"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir_root = '/content/drive/MyDrive/266-final/'\n",
        "# dir_data = '/content/drive/MyDrive/266-final/data/'\n",
        "# dir_data = '/content/drive/MyDrive/266-final/data/se21-t1-comp-lex-master/'\n",
        "dir_data = '/content/drive/MyDrive/266-final/data/266-comp-lex-master'\n",
        "dir_models = '/content/drive/MyDrive/266-final/models/'\n",
        "dir_results = '/content/drive/MyDrive/266-final/results/'"
      ],
      "metadata": {
        "id": "I3Tfro3Zzop5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandbai_api_key = \"5236444b7e96f5cf74038116d8c1efba161a4310\""
      ],
      "metadata": {
        "id": "HjZtvw5ScRDp"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tree /content/drive/MyDrive/266-final/data/266-comp-lex-master/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qw9f1Hol2UhL",
        "outputId": "d564d0e6-8dc6-4005-e009-2304cf642396"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01;34m/content/drive/MyDrive/266-final/data/266-comp-lex-master/\u001b[0m\n",
            "├── \u001b[01;34mfe-test-labels\u001b[0m\n",
            "│   ├── \u001b[00mtest_multi_df.csv\u001b[0m\n",
            "│   └── \u001b[00mtest_single_df.csv\u001b[0m\n",
            "├── \u001b[01;34mfe-train\u001b[0m\n",
            "│   ├── \u001b[00mtrain_multi_df.csv\u001b[0m\n",
            "│   └── \u001b[00mtrain_single_df.csv\u001b[0m\n",
            "├── \u001b[01;34mfe-trial-val\u001b[0m\n",
            "│   ├── \u001b[00mtrial_val_multi_df.csv\u001b[0m\n",
            "│   └── \u001b[00mtrial_val_single_df.csv\u001b[0m\n",
            "├── \u001b[01;34mtest-labels\u001b[0m\n",
            "│   ├── \u001b[00mlcp_multi_test.tsv\u001b[0m\n",
            "│   └── \u001b[00mlcp_single_test.tsv\u001b[0m\n",
            "├── \u001b[01;34mtrain\u001b[0m\n",
            "│   ├── \u001b[00mlcp_multi_train.tsv\u001b[0m\n",
            "│   └── \u001b[00mlcp_single_train.tsv\u001b[0m\n",
            "└── \u001b[01;34mtrial\u001b[0m\n",
            "    ├── \u001b[00mlcp_multi_trial.tsv\u001b[0m\n",
            "    └── \u001b[00mlcp_single_trial.tsv\u001b[0m\n",
            "\n",
            "6 directories, 12 files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -R /content/drive/MyDrive/266-final/data/266-comp-lex-master/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zgul33NlKkbV",
        "outputId": "2ad83a65-1302-4992-897f-40038207c1c0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/266-final/data/266-comp-lex-master/:\n",
            "fe-test-labels\tfe-train  fe-trial-val\ttest-labels  train  trial\n",
            "\n",
            "/content/drive/MyDrive/266-final/data/266-comp-lex-master/fe-test-labels:\n",
            "test_multi_df.csv  test_single_df.csv\n",
            "\n",
            "/content/drive/MyDrive/266-final/data/266-comp-lex-master/fe-train:\n",
            "train_multi_df.csv  train_single_df.csv\n",
            "\n",
            "/content/drive/MyDrive/266-final/data/266-comp-lex-master/fe-trial-val:\n",
            "trial_val_multi_df.csv\ttrial_val_single_df.csv\n",
            "\n",
            "/content/drive/MyDrive/266-final/data/266-comp-lex-master/test-labels:\n",
            "lcp_multi_test.tsv  lcp_single_test.tsv\n",
            "\n",
            "/content/drive/MyDrive/266-final/data/266-comp-lex-master/train:\n",
            "lcp_multi_train.tsv  lcp_single_train.tsv\n",
            "\n",
            "/content/drive/MyDrive/266-final/data/266-comp-lex-master/trial:\n",
            "lcp_multi_trial.tsv  lcp_single_trial.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tree /content/drive/MyDrive/266-final/data/266-comp-lex-master/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9f8sPUBdbVr",
        "outputId": "1e451625-0fb9-4338-924f-898e74c028ee"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01;34m/content/drive/MyDrive/266-final/data/266-comp-lex-master/\u001b[0m\n",
            "├── \u001b[01;34mfe-test-labels\u001b[0m\n",
            "│   ├── \u001b[00mtest_multi_df.csv\u001b[0m\n",
            "│   └── \u001b[00mtest_single_df.csv\u001b[0m\n",
            "├── \u001b[01;34mfe-train\u001b[0m\n",
            "│   ├── \u001b[00mtrain_multi_df.csv\u001b[0m\n",
            "│   └── \u001b[00mtrain_single_df.csv\u001b[0m\n",
            "├── \u001b[01;34mfe-trial-val\u001b[0m\n",
            "│   ├── \u001b[00mtrial_val_multi_df.csv\u001b[0m\n",
            "│   └── \u001b[00mtrial_val_single_df.csv\u001b[0m\n",
            "├── \u001b[01;34mtest-labels\u001b[0m\n",
            "│   ├── \u001b[00mlcp_multi_test.tsv\u001b[0m\n",
            "│   └── \u001b[00mlcp_single_test.tsv\u001b[0m\n",
            "├── \u001b[01;34mtrain\u001b[0m\n",
            "│   ├── \u001b[00mlcp_multi_train.tsv\u001b[0m\n",
            "│   └── \u001b[00mlcp_single_train.tsv\u001b[0m\n",
            "└── \u001b[01;34mtrial\u001b[0m\n",
            "    ├── \u001b[00mlcp_multi_trial.tsv\u001b[0m\n",
            "    └── \u001b[00mlcp_single_trial.tsv\u001b[0m\n",
            "\n",
            "6 directories, 12 files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import Data"
      ],
      "metadata": {
        "id": "oftTqvV8zojV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_names = [\n",
        "    \"train_single_df\",\n",
        "    \"train_multi_df\",\n",
        "    \"trial_val_single_df\",\n",
        "    \"trial_val_multi_df\",\n",
        "    \"test_single_df\",\n",
        "    \"test_multi_df\"\n",
        "]\n",
        "\n",
        "loaded_dataframes = {}\n",
        "\n",
        "for df_name in df_names:\n",
        "    if \"train\" in df_name:\n",
        "        subdir = \"fe-train\"\n",
        "    elif \"trial_val\" in df_name:\n",
        "        subdir = \"fe-trial-val\"\n",
        "    elif \"test\" in df_name:\n",
        "        subdir = \"fe-test-labels\"\n",
        "    else:\n",
        "        subdir = None\n",
        "\n",
        "    if subdir:\n",
        "        read_path = os.path.join(dir_data, subdir, f\"{df_name}.csv\")\n",
        "        loaded_df = pd.read_csv(read_path)\n",
        "        loaded_dataframes[df_name] = loaded_df\n",
        "        print(f\"Loaded {df_name} from {read_path}\")\n",
        "\n",
        "# for df_name, df in loaded_dataframes.items():\n",
        "#     print(f\"\\n>>> {df_name} shape: {df.shape}\")\n",
        "#     if 'binary_complexity' in df.columns:\n",
        "#         print(df['binary_complexity'].value_counts())\n",
        "#         print(df.info())\n",
        "#         print(df.head())\n",
        "\n",
        "for df_name, df in loaded_dataframes.items():\n",
        "    globals()[df_name] = df\n",
        "    print(f\"{df_name} loaded into global namespace.\")"
      ],
      "metadata": {
        "id": "73lV0P87eV-P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a4c7596-eceb-49ab-de5a-e24bf27c2aba"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded train_single_df from /content/drive/MyDrive/266-final/data/266-comp-lex-master/fe-train/train_single_df.csv\n",
            "Loaded train_multi_df from /content/drive/MyDrive/266-final/data/266-comp-lex-master/fe-train/train_multi_df.csv\n",
            "Loaded trial_val_single_df from /content/drive/MyDrive/266-final/data/266-comp-lex-master/fe-trial-val/trial_val_single_df.csv\n",
            "Loaded trial_val_multi_df from /content/drive/MyDrive/266-final/data/266-comp-lex-master/fe-trial-val/trial_val_multi_df.csv\n",
            "Loaded test_single_df from /content/drive/MyDrive/266-final/data/266-comp-lex-master/fe-test-labels/test_single_df.csv\n",
            "Loaded test_multi_df from /content/drive/MyDrive/266-final/data/266-comp-lex-master/fe-test-labels/test_multi_df.csv\n",
            "train_single_df loaded into global namespace.\n",
            "train_multi_df loaded into global namespace.\n",
            "trial_val_single_df loaded into global namespace.\n",
            "trial_val_multi_df loaded into global namespace.\n",
            "test_single_df loaded into global namespace.\n",
            "test_multi_df loaded into global namespace.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Functional tests pass, we can proceed with Baseline Modeling"
      ],
      "metadata": {
        "id": "8VsgfL5ZhO4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Experiment 1: Baseline Modeling"
      ],
      "metadata": {
        "id": "AryofObMaySF"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reminders:\n",
        "\n",
        "- Precision\n",
        "$$\n",
        "\\text{Precision} = \\frac{TP}{TP + FP}\n",
        "$$\n",
        "- Recall\n",
        "$$\n",
        "\\text{Recall} = \\frac{TP}{TP + FN}\n",
        "$$\n",
        "\n",
        "- Accuracy\n",
        "$$\n",
        "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
        "$$\n",
        "\n",
        "- F1 Score\n",
        "$$\n",
        "\\text{F1} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
        "$$\n",
        "\n",
        "- Cosine Similarity\n",
        "$$\n",
        "\\text{Cosine Similarity} = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\| \\, \\|\\mathbf{B}\\|}\n",
        "$$\n",
        "\n",
        "- Jaccard Similarity\n",
        "$$\n",
        "\\text{Jaccard Similarity} = \\frac{|A \\cap B|}{|A \\cup B|}\n",
        "$$\n",
        "\n",
        "- Overlap Similarity (Overlap Coefficient)\n",
        "$$\n",
        "\\text{Overlap Similarity} = \\frac{|A \\cap B|}{\\min(|A|, |B|)}\n",
        "$$\n",
        "\n",
        "- Dice Coefficient\n",
        "$$\n",
        "\\text{Dice Coefficient} = \\frac{2 \\, |A \\cap B|}{|A| + |B|}\n",
        "$$\n",
        "\n"
      ],
      "metadata": {
        "id": "zcq7Fwy9jazt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Naive Bayes"
      ],
      "metadata": {
        "id": "kXGkjXQGqeH8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### X = Sentence: contractions and no contractions"
      ],
      "metadata": {
        "id": "25Ph58CZtFJJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- sentence no contractions"
      ],
      "metadata": {
        "id": "MMDGr-ELvBZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_single_df\n",
        "val_df = trial_val_single_df\n",
        "\n",
        "vectorizer = TfidfVectorizer()  # just on 'sentence_no_contractions'\n",
        "X_train = vectorizer.fit_transform(train_df['sentence_no_contractions'])\n",
        "y_train = train_df['binary_complexity']\n",
        "\n",
        "X_val = vectorizer.transform(val_df['sentence_no_contractions'])\n",
        "y_val = val_df['binary_complexity']\n",
        "\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train, y_train)\n",
        "preds = clf.predict(X_val)\n",
        "print(classification_report(y_val, preds))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nd8mW5gfhV8j",
        "outputId": "9591843e-7358-48f7-ed42-c124ca1478dd"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.74      0.65       229\n",
            "           1       0.55      0.38      0.44       192\n",
            "\n",
            "    accuracy                           0.57       421\n",
            "   macro avg       0.57      0.56      0.55       421\n",
            "weighted avg       0.57      0.57      0.56       421\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- sentence with contractions"
      ],
      "metadata": {
        "id": "00DmH5T-vDp3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_single_df\n",
        "val_df = trial_val_single_df\n",
        "\n",
        "vectorizer = TfidfVectorizer()  # just on 'sentence'\n",
        "X_train = vectorizer.fit_transform(train_df['sentence'])\n",
        "y_train = train_df['binary_complexity']\n",
        "\n",
        "X_val = vectorizer.transform(val_df['sentence'])\n",
        "y_val = val_df['binary_complexity']\n",
        "\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train, y_train)\n",
        "preds = clf.predict(X_val)\n",
        "print(classification_report(y_val, preds))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0rDGC_Ur7Tc",
        "outputId": "492ecfce-45ae-4fe7-f52b-ae28326e3708"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.74      0.65       229\n",
            "           1       0.55      0.38      0.44       192\n",
            "\n",
            "    accuracy                           0.57       421\n",
            "   macro avg       0.57      0.56      0.55       421\n",
            "weighted avg       0.57      0.57      0.56       421\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- sentence no contractions"
      ],
      "metadata": {
        "id": "s90SKXECvFdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_multi_df\n",
        "val_df = trial_val_multi_df\n",
        "\n",
        "vectorizer = TfidfVectorizer()  # just on 'sentence_no_contractions'\n",
        "X_train = vectorizer.fit_transform(train_df['sentence_no_contractions'])\n",
        "y_train = train_df['binary_complexity']\n",
        "\n",
        "X_val = vectorizer.transform(val_df['sentence_no_contractions'])\n",
        "y_val = val_df['binary_complexity']\n",
        "\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train, y_train)\n",
        "preds = clf.predict(X_val)\n",
        "print(classification_report(y_val, preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5krsiE6ohV6F",
        "outputId": "2038b7d3-4f2c-4b60-ed90-c48c6e76c948"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.52      0.67      0.58        48\n",
            "           1       0.57      0.41      0.48        51\n",
            "\n",
            "    accuracy                           0.54        99\n",
            "   macro avg       0.54      0.54      0.53        99\n",
            "weighted avg       0.54      0.54      0.53        99\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- sentence with contractions"
      ],
      "metadata": {
        "id": "gSARiKJUvJlO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_multi_df\n",
        "val_df = trial_val_multi_df\n",
        "\n",
        "vectorizer = TfidfVectorizer()  # just on 'sentence'\n",
        "X_train = vectorizer.fit_transform(train_df['sentence'])\n",
        "y_train = train_df['binary_complexity']\n",
        "\n",
        "X_val = vectorizer.transform(val_df['sentence'])\n",
        "y_val = val_df['binary_complexity']\n",
        "\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train, y_train)\n",
        "preds = clf.predict(X_val)\n",
        "print(classification_report(y_val, preds))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDhdExbrhV3n",
        "outputId": "b92f539d-a88c-4a6d-e42b-28ff543483d0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.52      0.67      0.58        48\n",
            "           1       0.57      0.41      0.48        51\n",
            "\n",
            "    accuracy                           0.54        99\n",
            "   macro avg       0.54      0.54      0.53        99\n",
            "weighted avg       0.54      0.54      0.53        99\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Score is higher than expected for a Naive Bayes model**\n",
        "- **There is no difference in performance when using the input sequence of the sentence with and without contractions**"
      ],
      "metadata": {
        "id": "8ZGQHDtwsMwa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### X = pos_sequence: Part-of-Speech Tags"
      ],
      "metadata": {
        "id": "kmm6vDVPsm6a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- POS Tags: Extracts the part-of-speech (POS) tags for each token (e.g., \"DET\", \"NOUN\", \"VERB\")."
      ],
      "metadata": {
        "id": "UAkALOTjuG36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_single_df\n",
        "val_df = trial_val_single_df\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train = vectorizer.fit_transform(train_df['pos_sequence'])\n",
        "y_train = train_df['binary_complexity']\n",
        "\n",
        "X_val = vectorizer.transform(val_df['pos_sequence'])\n",
        "y_val = val_df['binary_complexity']\n",
        "\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train, y_train)\n",
        "preds = clf.predict(X_val)\n",
        "print(classification_report(y_val, preds))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_j5rivRdsbaQ",
        "outputId": "4e5f1385-f88d-4e33-c199-fcc992be1c67"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.67      0.63       229\n",
            "           1       0.54      0.46      0.50       192\n",
            "\n",
            "    accuracy                           0.57       421\n",
            "   macro avg       0.57      0.57      0.56       421\n",
            "weighted avg       0.57      0.57      0.57       421\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_multi_df\n",
        "val_df = trial_val_multi_df\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train = vectorizer.fit_transform(train_df['pos_sequence'])\n",
        "y_train = train_df['binary_complexity']\n",
        "\n",
        "X_val = vectorizer.transform(val_df['pos_sequence'])\n",
        "y_val = val_df['binary_complexity']\n",
        "\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train, y_train)\n",
        "preds = clf.predict(X_val)\n",
        "print(classification_report(y_val, preds))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJUA-jU3tBKj",
        "outputId": "e0d3743d-233e-43f6-eccc-0e4b3e4e801b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.54      0.56        48\n",
            "           1       0.59      0.63      0.61        51\n",
            "\n",
            "    accuracy                           0.59        99\n",
            "   macro avg       0.59      0.58      0.58        99\n",
            "weighted avg       0.59      0.59      0.59        99\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Part of Speech tags outperform raw input sequence"
      ],
      "metadata": {
        "id": "y9fD5Js7t8Ez"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### X = dep_sequence: Dependency Tags\n",
        "\n",
        "- Dependency Tags: Extracts the syntactic dependency labels for each token (e.g., \"det\", \"nsubj\", \"ROOT\")."
      ],
      "metadata": {
        "id": "N9puBjbrtktC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_single_df\n",
        "val_df = trial_val_single_df\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train = vectorizer.fit_transform(train_df['dep_sequence'])\n",
        "y_train = train_df['binary_complexity']\n",
        "\n",
        "X_val = vectorizer.transform(val_df['dep_sequence'])\n",
        "y_val = val_df['binary_complexity']\n",
        "\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train, y_train)\n",
        "preds = clf.predict(X_val)\n",
        "print(classification_report(y_val, preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0uXfJPLsbX0",
        "outputId": "404e8949-6044-4927-c8f7-cc6780a31f7b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.60      0.60       229\n",
            "           1       0.53      0.54      0.54       192\n",
            "\n",
            "    accuracy                           0.57       421\n",
            "   macro avg       0.57      0.57      0.57       421\n",
            "weighted avg       0.57      0.57      0.57       421\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_multi_df\n",
        "val_df = trial_val_multi_df\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train = vectorizer.fit_transform(train_df['dep_sequence'])\n",
        "y_train = train_df['binary_complexity']\n",
        "\n",
        "X_val = vectorizer.transform(val_df['dep_sequence'])\n",
        "y_val = val_df['binary_complexity']\n",
        "\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train, y_train)\n",
        "preds = clf.predict(X_val)\n",
        "print(classification_report(y_val, preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmVkGchCuuj2",
        "outputId": "9428c46e-f472-4acd-d213-28b7f6f36f48"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.51      0.46      0.48        48\n",
            "           1       0.54      0.59      0.56        51\n",
            "\n",
            "    accuracy                           0.53        99\n",
            "   macro avg       0.52      0.52      0.52        99\n",
            "weighted avg       0.52      0.53      0.52        99\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### X = morph_sequence: Morphological Features\n",
        "- For each token, the morphological attributes have been retrieved for each token"
      ],
      "metadata": {
        "id": "hlE9BbXuuYQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_single_df\n",
        "val_df = trial_val_single_df\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train = vectorizer.fit_transform(train_df['morph_sequence'])\n",
        "y_train = train_df['binary_complexity']\n",
        "\n",
        "X_val = vectorizer.transform(val_df['morph_sequence'])\n",
        "y_val = val_df['binary_complexity']\n",
        "\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train, y_train)\n",
        "preds = clf.predict(X_val)\n",
        "print(classification_report(y_val, preds))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpy5Kh8SsbTR",
        "outputId": "4a69f0f3-d20a-48b2-edbf-cc7fae94cb78"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.59      0.60       229\n",
            "           1       0.53      0.57      0.55       192\n",
            "\n",
            "    accuracy                           0.58       421\n",
            "   macro avg       0.58      0.58      0.58       421\n",
            "weighted avg       0.58      0.58      0.58       421\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_multi_df\n",
        "val_df = trial_val_multi_df\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train = vectorizer.fit_transform(train_df['morph_sequence'])\n",
        "y_train = train_df['binary_complexity']\n",
        "\n",
        "X_val = vectorizer.transform(val_df['morph_sequence'])\n",
        "y_val = val_df['binary_complexity']\n",
        "\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train, y_train)\n",
        "preds = clf.predict(X_val)\n",
        "print(classification_report(y_val, preds))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SS92gFd1sbQe",
        "outputId": "0fd0de53-8832-4e58-b7db-df9c02615dd2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.52      0.57        48\n",
            "           1       0.61      0.71      0.65        51\n",
            "\n",
            "    accuracy                           0.62        99\n",
            "   macro avg       0.62      0.61      0.61        99\n",
            "weighted avg       0.62      0.62      0.61        99\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baseline Experiment Results\n",
        "\n",
        "The table below summarizes the evaluation metrics for our Naive Bayes experiments. We report results for both sentence inputs (with and without contractions) as well as for the linguistic feature representations: Part-of-Speech tags (POS), Dependency tags, and Morphological features. Results are provided separately for the *Single* and *Multi* datasets. **Our Preferred Evaluation Metric of Interest is F1 Score**.\n",
        "\n",
        "| **Input Type**                   | **Dataset** | **Accuracy** | **Precision** | **Recall** | **F1 Score** |\n",
        "|----------------------------------|-------------|--------------|---------------|------------|--------------|\n",
        "| Sentence (with contractions)     | Single      | 57%          | 57%           | 57%        | 57%          |\n",
        "| Sentence (without contractions)  | Single      | 57%          | 57%           | 57%        | 57%          |\n",
        "| Sentence (with contractions)     | Multi       | 54%          | 54%           | 54%        | 54%          |\n",
        "| Sentence (without contractions)  | Multi       | 54%          | 54%           | 54%        | 54%          |\n",
        "| POS Tags (pos_sequence)      | Single      | 57%          | 57%           | 57%        | 57%          |\n",
        "| POS Tags (pos_sequence)      | Multi       | 59%          | 59%           | 59%        | 59%          |\n",
        "| Dependency Tags (dep_sequence) | Single    | 57%          | 57%           | 57%        | 57%          |\n",
        "| Dependency Tags (dep_sequence) | Multi     | 52%          | 52%           | 52%        | 52%          |\n",
        "| Morphological Features (morph_sequence) | Single | 58%    | 58%           | 58%        | 58%          |\n",
        "| Morphological Features (morph_sequence) | Multi  | 62%    | 62%           | 62%        | 62%          |\n",
        "\n",
        "*Note:* The metrics shown above are the weighted averages derived from Trial_Val.\n",
        "\n",
        "\n",
        "\n",
        "#### Evaluation\n",
        "\n",
        "- **Raw Sentence Input:** Both with and without contractions, the single-dataset experiment shows a macro F1-score of 0.57, while the multi-dataset experiment yields a lower F1-score (0.54). This suggests that for raw text, model performance degrades on the multi-label version. **While there is no contextual difference between in the contexts between the single and multi versions, the binary_complexity is different, as the complexity scores derived from the 'complex unigram and bigram tokens' in both the single and multi splits of the datasets achieved different scores, and thus different medians (from which we derived our binarized value).**\n",
        "\n",
        "- **POS Tags:** Using part-of-speech tag sequences produces results similar to raw text on the single dataset (F1 = 0.57) and even slightly better performance on the multi dataset (F1 = 0.59).\n",
        "\n",
        "- **Dependency Tags:** Dependency label sequences perform on par with the other features in the single-dataset setting (F1 = 0.57) but drop to an F1-score of 0.52 on the multi dataset, indicating less robustness for this representation in that setting.\n",
        "\n",
        "- **Morphological Features:** On the single dataset, morphological features give a modest improvement (F1 = 0.58) over raw text. Notably, on the multi dataset, they yield the highest performance (F1 = 0.62), suggesting that despite there being no contextual difference between the two, Naive Bayes' capacity to split the complexity of the input sequence is more aligned with the median threshold of the multi-version split of the data. <i>However, it should be noted that the multi-split for trial_val is literally only 99 records, so I expect that these performance metrics will drop substantially on the test set</i>\n",
        "\n",
        "- **Hyperparameter Tuning:** Naive Bayes was used in a fairly vanilla manner, not reflected in this notebook were some experiments done with varying alphas (i.e. Laplace Smoothing Values)—these led to effectively no difference in average F1 Score results.\n",
        "\n",
        "Overall, these results indicate that while raw text and simple POS tags are competitive, the morphological feature representation provides an edge—especially in the multi dataset scenario. **This indicates keeping these additional features on-hand for transformers-based ablations may be a good call.**\n"
      ],
      "metadata": {
        "id": "pGyR5awNxlm8"
      }
    }
  ]
}