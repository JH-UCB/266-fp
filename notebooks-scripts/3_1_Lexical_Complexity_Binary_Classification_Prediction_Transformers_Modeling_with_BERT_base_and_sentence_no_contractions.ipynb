{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@title Install Packages"
      ],
      "metadata": {
        "id": "xYRurcbg2fuq"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWeMhKnlvi9F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd82fbfa-41fb-41a0-9ecf-3c48bfc53968"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
            "    return func(self, options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/commands/install.py\", line 447, in run\n",
            "^C\n",
            "^C\n",
            "^C\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers\n",
        "!pip install -q torchinfo\n",
        "!pip install -q datasets\n",
        "!pip install -q evaluate\n",
        "!pip install -q nltk\n",
        "!pip install -q contractions\n",
        "!pip install -q hf_xet\n",
        "!pip install -q sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update\n",
        "! sudo apt-get install tree"
      ],
      "metadata": {
        "id": "rX3VqZu11-s_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Imports\n",
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "import contractions\n",
        "\n",
        "import evaluate\n",
        "import transformers\n",
        "import torch\n",
        "\n",
        "from torchinfo import summary\n",
        "\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, TrainingArguments, Trainer, BertConfig, BertForSequenceClassification\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import sklearn\n",
        "\n",
        "import spacy\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, precision_recall_fscore_support, accuracy_score\n",
        "\n",
        "import sentencepiece\n",
        "\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "3wEgNBR6zosA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Mount Google Drive"
      ],
      "metadata": {
        "id": "rSP7bIn12YU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "oWKPq7h01cXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dir_root = '/content/drive/MyDrive/266-final/'\n",
        "# dir_data = '/content/drive/MyDrive/266-final/data/'\n",
        "# dir_data = '/content/drive/MyDrive/266-final/data/se21-t1-comp-lex-master/'\n",
        "dir_data = '/content/drive/MyDrive/266-final/data/266-comp-lex-master'\n",
        "dir_models = '/content/drive/MyDrive/266-final/models/'\n",
        "dir_results = '/content/drive/MyDrive/266-final/results/'"
      ],
      "metadata": {
        "id": "I3Tfro3Zzop5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandbai_api_key = \"5236444b7e96f5cf74038116d8c1efba161a4310\""
      ],
      "metadata": {
        "id": "HjZtvw5ScRDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tree /content/drive/MyDrive/266-final/data/266-comp-lex-master/"
      ],
      "metadata": {
        "id": "Qw9f1Hol2UhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -R /content/drive/MyDrive/266-final/data/266-comp-lex-master/"
      ],
      "metadata": {
        "id": "Zgul33NlKkbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tree /content/drive/MyDrive/266-final/data/266-comp-lex-master/"
      ],
      "metadata": {
        "id": "G9f8sPUBdbVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import Data"
      ],
      "metadata": {
        "id": "oftTqvV8zojV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_names = [\n",
        "    \"train_single_df\",\n",
        "    \"train_multi_df\",\n",
        "    \"trial_val_single_df\",\n",
        "    \"trial_val_multi_df\",\n",
        "    \"test_single_df\",\n",
        "    \"test_multi_df\"\n",
        "]\n",
        "\n",
        "loaded_dataframes = {}\n",
        "\n",
        "for df_name in df_names:\n",
        "    if \"train\" in df_name:\n",
        "        subdir = \"fe-train\"\n",
        "    elif \"trial_val\" in df_name:\n",
        "        subdir = \"fe-trial-val\"\n",
        "    elif \"test\" in df_name:\n",
        "        subdir = \"fe-test-labels\"\n",
        "    else:\n",
        "        subdir = None\n",
        "\n",
        "    if subdir:\n",
        "        read_path = os.path.join(dir_data, subdir, f\"{df_name}.csv\")\n",
        "        loaded_df = pd.read_csv(read_path)\n",
        "        loaded_dataframes[df_name] = loaded_df\n",
        "        print(f\"Loaded {df_name} from {read_path}\")\n",
        "\n",
        "# for df_name, df in loaded_dataframes.items():\n",
        "#     print(f\"\\n>>> {df_name} shape: {df.shape}\")\n",
        "#     if 'binary_complexity' in df.columns:\n",
        "#         print(df['binary_complexity'].value_counts())\n",
        "#         print(df.info())\n",
        "#         print(df.head())\n",
        "\n",
        "for df_name, df in loaded_dataframes.items():\n",
        "    globals()[df_name] = df\n",
        "    print(f\"{df_name} loaded into global namespace.\")"
      ],
      "metadata": {
        "id": "73lV0P87eV-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Functional tests pass, we can proceed with Baseline Modeling"
      ],
      "metadata": {
        "id": "8VsgfL5ZhO4z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiments with Transformers Models"
      ],
      "metadata": {
        "id": "kxZvACQZQu61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def get_model_and_tokenizer(model_name: str):\n",
        "#     \"\"\"\n",
        "#     Loads the specified pretrained model & tokenizer for classification.\n",
        "#     \"\"\"\n",
        "#     tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "#     model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "#     return model, tokenizer\n",
        "\n",
        "# new prod version to support local model checkpoints, to be used after experiment 1.0\n",
        "def get_model_and_tokenizer(\n",
        "    remote_model_name: str = None,\n",
        "    local_model_path: str = None\n",
        "):\n",
        "    \"\"\"\n",
        "    Loads the model & tokenizer for classification.\n",
        "    If 'local_model_path' is specified, load from that path.\n",
        "    Otherwise, fall back to 'remote_model_name'.\n",
        "    \"\"\"\n",
        "    from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "    if local_model_path:\n",
        "        # Local load\n",
        "        print(f\"Loading from local path: {local_model_path}\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(local_model_path)\n",
        "    elif remote_model_name:\n",
        "        # Load from HF Hub\n",
        "        print(f\"Loading from Hugging Face model: {remote_model_name}\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(remote_model_name)\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(remote_model_name)\n",
        "    else:\n",
        "        raise ValueError(\"You must provide either a remote_model_name or a local_model_path!\")\n",
        "\n",
        "    return model, tokenizer"
      ],
      "metadata": {
        "id": "-rpwRVNF_e56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def freeze_unfreeze_layers(model, layers_to_unfreeze=None):\n",
        "    \"\"\"\n",
        "    Toggles requires_grad = False for all parameters\n",
        "    except for those whose names contain any string in layers_to_unfreeze.\n",
        "    By default, always unfreeze classifier/heads.\n",
        "    \"\"\"\n",
        "    if layers_to_unfreeze is None:\n",
        "        layers_to_unfreeze = [\"classifier.\", \"pooler.\"]\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        # If any layer substring matches, we unfreeze\n",
        "        if any(substring in name for substring in layers_to_unfreeze):\n",
        "            param.requires_grad = True\n",
        "        else:\n",
        "            param.requires_grad = False"
      ],
      "metadata": {
        "id": "a7OVfxB__e3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def encode_examples(examples, tokenizer, text_col, max_length=256):\n",
        "    \"\"\"\n",
        "    Tokenizes a batch of texts from 'examples[text_col]' using the given tokenizer.\n",
        "    Returns a dict with 'input_ids', 'attention_mask', etc.\n",
        "    \"\"\"\n",
        "    texts = examples[text_col]\n",
        "    encoded = tokenizer(\n",
        "        texts,\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=max_length\n",
        "    )\n",
        "    return encoded"
      ],
      "metadata": {
        "id": "BtVWXxqb_e0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(df, tokenizer, text_col, label_col, max_length=256):\n",
        "    \"\"\"\n",
        "    Converts a Pandas DataFrame to a Hugging Face Dataset,\n",
        "    then applies 'encode_examples' to tokenize.\n",
        "    \"\"\"\n",
        "    # Convert to HF Dataset\n",
        "    dataset = Dataset.from_pandas(df)\n",
        "\n",
        "    # Map the encode function\n",
        "    dataset = dataset.map(\n",
        "        lambda batch: encode_examples(batch, tokenizer, text_col, max_length),\n",
        "        batched=True\n",
        "    )\n",
        "\n",
        "    # Rename the label column to 'labels' for HF Trainer\n",
        "    dataset = dataset.rename_column(label_col, \"labels\")\n",
        "    # HF often requires removing any columns that cannot be converted or are not needed\n",
        "    dataset.set_format(type='torch',\n",
        "                       columns=['input_ids', 'attention_mask', 'labels'])\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "YmynPX-5i5HL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"\n",
        "    Computes classification metrics, including accuracy, precision, recall, and F1.\n",
        "    \"\"\"\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=1)\n",
        "\n",
        "    metric_accuracy  = evaluate.load(\"accuracy\")\n",
        "    metric_precision = evaluate.load(\"precision\")\n",
        "    metric_recall    = evaluate.load(\"recall\")\n",
        "    metric_f1        = evaluate.load(\"f1\")\n",
        "\n",
        "    accuracy_result  = metric_accuracy.compute(predictions=preds, references=labels)\n",
        "    precision_result = metric_precision.compute(predictions=preds, references=labels, average=\"binary\")\n",
        "    recall_result    = metric_recall.compute(predictions=preds, references=labels, average=\"binary\")\n",
        "    f1_result        = metric_f1.compute(predictions=preds, references=labels, average=\"binary\")\n",
        "\n",
        "    return {\n",
        "        \"accuracy\"       : accuracy_result[\"accuracy\"],\n",
        "        \"precision\": precision_result[\"precision\"],\n",
        "        \"recall\"   : recall_result[\"recall\"],\n",
        "        \"f1\"       : f1_result[\"f1\"]\n",
        "    }"
      ],
      "metadata": {
        "id": "ze7GiYRP_ewQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experiment Design"
      ],
      "metadata": {
        "id": "5w231tlmrh_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Experiment Parameters\n",
        "\n",
        "named_model = \"bert-base-cased\"\n",
        "# named_model = \"roberta-base\"\n",
        "# named_model = \"bert-large\"\n",
        "# named_model = \"roberta-large\"\n",
        "# named_model = \"\" # modern bert\n",
        "\n",
        "# learning_rate = 1e-3\n",
        "# learning_rate = 1e-4\n",
        "# learning_rate = 1e-5\n",
        "# learning_rate = 5e-6\n",
        "learning_rate = 5e-7\n",
        "# learning_rate = 5e-8\n",
        "\n",
        "# num_epochs = 3\n",
        "num_epochs = 5\n",
        "# num_epochs = 10\n",
        "# num_epochs = 15\n",
        "# num_epochs = 20\n",
        "\n",
        "length_max = 128\n",
        "# length_max = 256\n",
        "# length_max = 348\n",
        "# length_max = 512\n",
        "\n",
        "# size_batch = 1\n",
        "# size_batch = 4\n",
        "size_batch = 8\n",
        "# size_batch = 16\n",
        "# size_batch = 24\n",
        "# size_batch = 32\n",
        "\n",
        "regularization_weight_decay = 0\n",
        "# regularization_weight_decay = 0.1\n",
        "# regularization_weight_decay = 0.5\n",
        "\n",
        "# dropout???\n",
        "\n",
        "# layers to freeze and unfreeze?\n",
        "\n",
        "y_col = \"binary_complexity\"\n",
        "# y_col = \"complexity\"\n",
        "\n",
        "x_task = \"single\"\n",
        "# x_task = \"multi\"\n",
        "\n",
        "# x_col = \"sentence\"\n",
        "x_col = \"sentence_no_contractions\"\n",
        "# x_col = \"pos_sequence\"\n",
        "# x_col = \"dep_sequence\"\n",
        "# x_col = \"morph_sequence\"\n",
        "\n",
        "if x_task == \"single\":\n",
        "    df_train = train_single_df\n",
        "    df_val   = trial_val_single_df\n",
        "    df_test  = test_single_df\n",
        "else:\n",
        "    df_train = train_multi_df\n",
        "    df_val   = trial_val_multi_df\n",
        "    df_test  = test_multi_df\n"
      ],
      "metadata": {
        "id": "PjPcND4vrgOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_transformer_model(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    train_dataset,\n",
        "    val_dataset,\n",
        "    output_dir=dir_results,\n",
        "    num_epochs=num_epochs,\n",
        "    batch_size=size_batch,\n",
        "    lr=learning_rate,\n",
        "    weight_decay=regularization_weight_decay\n",
        "):\n",
        "    \"\"\"\n",
        "    Sets up a Trainer and trains the model for 'num_epochs' using the given dataset.\n",
        "    Returns the trained model and the Trainer object for possible re-use or analysis.\n",
        "    \"\"\"\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=num_epochs,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"no\",\n",
        "        logging_strategy=\"epoch\",\n",
        "        learning_rate=lr,\n",
        "        weight_decay=weight_decay,\n",
        "        report_to=[\"none\"],  # or \"wandb\"\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        tokenizer=tokenizer,  # optional\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    return model, trainer"
      ],
      "metadata": {
        "id": "b-kyadzyrgHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.0: from pretrained bert-base-cased single task 1"
      ],
      "metadata": {
        "id": "O_g9-bLdVBD_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Inspection"
      ],
      "metadata": {
        "id": "UWNP_mfRZFJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"model checkpoints:\", dir_models)\n",
        "!ls /content/drive/MyDrive/266-final/models/"
      ],
      "metadata": {
        "id": "s5raqYA8p8zQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Model & Tokenizer\n",
        "# model, tokenizer = get_model_and_tokenizer(named_model) # deprecated argument structure\n",
        "model, tokenizer = get_model_and_tokenizer(\"/content/drive/MyDrive/266-final/models/....\") # proposed argument usage for checkpointed models\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    print(name)\n",
        "\n",
        "print(\"=============\")\n",
        "print(named_model, \":\")\n",
        "print(\"=============\")\n",
        "print(model)\n",
        "print(\"=============\")\n",
        "print(model.config)\n",
        "print(\"=============\")\n",
        "print(\"num_parameters:\", model.num_parameters())\n",
        "print(\"=============\")\n",
        "print(\"num_trainable_parameters:\", model.num_parameters(only_trainable=True))"
      ],
      "metadata": {
        "id": "dVZ8rPGOVT5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Layer Configuration"
      ],
      "metadata": {
        "id": "9xKfKrycZH8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze/Unfreeze Layers & Additional Configuration Parameters\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "layers_to_unfreeze = [\n",
        "    \"bert.encoder.layer.9.\",\n",
        "    \"bert.encoder.layer.10.\",\n",
        "    \"bert.encoder.layer.11.\",\n",
        "    \"pooler.\",\n",
        "    \"classifier.\",\n",
        "]\n",
        "\n",
        "freeze_unfreeze_layers(model, layers_to_unfreeze=layers_to_unfreeze)\n",
        "\n",
        "\n",
        "bert_config = BertConfig(\n",
        "    # vocab_size=28996,\n",
        "    hidden_size=768,\n",
        "    # num_hidden_layers=12,\n",
        "    # num_attention_heads=12,\n",
        "    # intermediate_size=3072,\n",
        "    intermediate_size=6144,\n",
        "    # max_position_embeddings=512,\n",
        "    type_vocab_size=2,\n",
        "\n",
        "    hidden_dropout_prob=0.1,\n",
        "    attention_probs_dropout_prob=0.1,\n",
        "    # classifier_dropout=None,\n",
        "    # initializer_range=0.02,\n",
        "    # layer_norm_eps=1e-12,\n",
        "\n",
        "    hidden_act=\"gelu\",\n",
        "    gradient_checkpointing=True,\n",
        "    position_embedding_type=\"absolute\",\n",
        "    use_cache=True,\n",
        "    pad_token_id=0\n",
        ")\n",
        "\n",
        "model.bert.pooler.activation = nn.ReLU() # Tanh() replaced as the pooler layer activation function\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    print(name, \"requires_grad=\", param.requires_grad)\n",
        "\n",
        "print(\"\\nLayers that are 'True' are trainable. 'False' are frozen.\")\n",
        "\n",
        "print(\"=============\")\n",
        "print(named_model, \":\")\n",
        "print(\"=============\")\n",
        "print(model)\n",
        "print(\"=============\")\n",
        "print(model.config)\n",
        "print(\"=============\")\n",
        "print(\"num_parameters:\", model.num_parameters())\n",
        "print(\"=============\")\n",
        "print(\"num_trainable_parameters:\", model.num_parameters(only_trainable=True))"
      ],
      "metadata": {
        "id": "UTv7BTh5VXQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dataset Preparation"
      ],
      "metadata": {
        "id": "s0vgf-iJZQzv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize & Prepare Datasets\n",
        "\n",
        "train_data_hf = prepare_dataset(\n",
        "    df_train,\n",
        "    tokenizer,\n",
        "    text_col=x_col,\n",
        "    label_col=y_col,\n",
        "    max_length=length_max\n",
        ")\n",
        "\n",
        "val_data_hf = prepare_dataset(\n",
        "    df_val,\n",
        "    tokenizer,\n",
        "    text_col=x_col,\n",
        "    label_col=y_col,\n",
        "    max_length=length_max\n",
        ")\n",
        "\n",
        "test_data_hf = prepare_dataset(\n",
        "    df_test,\n",
        "    tokenizer,\n",
        "    text_col=x_col,\n",
        "    label_col=y_col,\n",
        "    max_length=length_max\n",
        "\n",
        ")\n",
        "\n",
        "print(\"Datasets prepared. Sample from train_data_hf:\\n\", train_data_hf[10])\n",
        "print(\"Datasets prepared. Sample from train_data_hf:\\n\", val_data_hf[10])\n",
        "print(\"Datasets prepared. Sample from train_data_hf:\\n\", test_data_hf[10])"
      ],
      "metadata": {
        "id": "bMQMoy5rcdLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.0 Results"
      ],
      "metadata": {
        "id": "9rUaWDeUw_Bu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train & Evaluate\n",
        "\n",
        "trained_model, trainer_obj = train_transformer_model(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_data_hf,\n",
        "    val_dataset=val_data_hf,\n",
        "    output_dir=dir_results,\n",
        "    num_epochs=num_epochs,\n",
        "    batch_size=size_batch,\n",
        "    lr=learning_rate,\n",
        "    weight_decay=regularization_weight_decay\n",
        ")\n",
        "\n",
        "metrics = trainer_obj.evaluate()\n",
        "print(\"Validation metrics:\", metrics)\n",
        "\n",
        "test_metrics = trainer_obj.evaluate(test_data_hf) if test_data_hf else None\n",
        "print(\"Test metrics:\", test_metrics)"
      ],
      "metadata": {
        "id": "-entrH9lieD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Experiment configuration used with this experiment:\")\n",
        "print(\"model used:\", named_model)\n",
        "print(\"learning rate used:\", learning_rate)\n",
        "print(\"number of epochs:\", num_epochs)\n",
        "print(\"maximum sequence length:\", length_max)\n",
        "print(\"batch size used:\", size_batch)\n",
        "print(\"regularization value:\", regularization_weight_decay)\n",
        "print(\"outcome variable:\", y_col)\n",
        "print(\"task:\", x_task)\n",
        "print(\"input column:\", x_col)"
      ],
      "metadata": {
        "id": "x8hht2anjdDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save model checkpoint\n",
        "\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "model_save_path = os.path.join(dir_models, f\"{x_task}_{named_model}_{y_col}_{timestamp}\")\n",
        "\n",
        "trainer_obj.save_model(model_save_path)\n",
        "print(f\"Model checkpoint saved to: {model_save_path}\")"
      ],
      "metadata": {
        "id": "dSTfG1W6lM9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experiment 1.1: from checkpoint bert-base-cased single task 1"
      ],
      "metadata": {
        "id": "iwW4R6lWk5Th"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Experiment Parameters\n",
        "\n",
        "named_model = \"bert-base-cased\"\n",
        "# named_model = \"roberta-base\"\n",
        "# named_model = \"bert-large\"\n",
        "# named_model = \"roberta-large\"\n",
        "# named_model = \"\" # modern bert\n",
        "\n",
        "# learning_rate = 1e-3\n",
        "# learning_rate = 1e-4\n",
        "# learning_rate = 1e-5\n",
        "# learning_rate = 5e-6\n",
        "# learning_rate = 5e-7\n",
        "learning_rate = 1e-8\n",
        "\n",
        "# num_epochs = 3\n",
        "num_epochs = 5\n",
        "# num_epochs = 10\n",
        "# num_epochs = 15\n",
        "# num_epochs = 20\n",
        "\n",
        "length_max = 128\n",
        "# length_max = 256\n",
        "# length_max = 348\n",
        "# length_max = 512\n",
        "\n",
        "# size_batch = 1\n",
        "size_batch = 4\n",
        "# size_batch = 8\n",
        "# size_batch = 16\n",
        "# size_batch = 24\n",
        "# size_batch = 32\n",
        "\n",
        "# regularization_weight_decay = 0\n",
        "regularization_weight_decay = 0.1\n",
        "# regularization_weight_decay = 0.5\n",
        "\n",
        "y_col = \"binary_complexity\"\n",
        "# y_col = \"complexity\"\n",
        "\n",
        "x_task = \"single\"\n",
        "# x_task = \"multi\"\n",
        "\n",
        "# x_col = \"sentence\"\n",
        "x_col = \"sentence_no_contractions\"\n",
        "# x_col = \"pos_sequence\"\n",
        "# x_col = \"dep_sequence\"\n",
        "# x_col = \"morph_sequence\"\n",
        "\n",
        "if x_task == \"single\":\n",
        "    df_train = train_single_df\n",
        "    df_val   = trial_val_single_df\n",
        "    df_test  = test_single_df\n",
        "else:\n",
        "    df_train = train_multi_df\n",
        "    df_val   = trial_val_multi_df\n",
        "    df_test  = test_multi_df"
      ],
      "metadata": {
        "id": "Ew3YUukFlGUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Model & Tokenizer\n",
        "model, tokenizer = get_model_and_tokenizer(named_model) # deprecated argument structure\n",
        "# model, tokenizer = get_model_and_tokenizer(\"/content/drive/MyDrive/266-final/models/bert-base-cased_20250407_232900\") # proposed argument usage for checkpointed models\n",
        "\n",
        "# for name, param in model.named_parameters():\n",
        "#     print(name)\n",
        "\n",
        "print(\"=============\")\n",
        "print(named_model, \":\")\n",
        "print(\"=============\")\n",
        "# print(model)\n",
        "print(\"=============\")\n",
        "print(model.config)\n",
        "# print(\"=============\")\n",
        "# print(\"num_parameters:\", model.num_parameters())\n",
        "# print(\"=============\")\n",
        "# print(\"num_trainable_parameters:\", model.num_parameters(only_trainable=True))"
      ],
      "metadata": {
        "id": "-ETCjJYpsJqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze/Unfreeze Layers & Additional Configuration Parameters\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "layers_to_unfreeze = [\n",
        "    \"bert.embeddings.\",\n",
        "    \"bert.encoder.layer.0.\",\n",
        "    \"bert.encoder.layer.1.\",\n",
        "    \"bert.encoder.layer.9.\",\n",
        "    \"bert.encoder.layer.9.\",\n",
        "    \"bert.encoder.layer.10.\",\n",
        "    \"bert.encoder.layer.11.\",\n",
        "    \"bert.pooler.\",\n",
        "    \"classifier.\",\n",
        "]\n",
        "\n",
        "freeze_unfreeze_layers(model, layers_to_unfreeze=layers_to_unfreeze)\n",
        "\n",
        "\n",
        "bert_config = BertConfig(\n",
        "    # vocab_size=28996,\n",
        "    hidden_size=768,\n",
        "    # num_hidden_layers=12,\n",
        "    # num_attention_heads=12,\n",
        "    intermediate_size=6144,\n",
        "    # max_position_embeddings=512,\n",
        "    type_vocab_size=2,\n",
        "\n",
        "    hidden_dropout_prob=0.1,\n",
        "    attention_probs_dropout_prob=0.1,\n",
        "    # classifier_dropout=None,\n",
        "    # initializer_range=0.02,\n",
        "    # layer_norm_eps=1e-12,\n",
        "\n",
        "    hidden_act=\"gelu\",\n",
        "    gradient_checkpointing=True,\n",
        "    position_embedding_type=\"absolute\",\n",
        "    use_cache=True,\n",
        "    pad_token_id=0\n",
        ")\n",
        "\n",
        "model.bert.pooler.activation = nn.ReLU() # Tanh() replaced as the pooler layer activation function\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    print(name, \"requires_grad=\", param.requires_grad)\n",
        "\n",
        "print(\"\\nLayers that are 'True' are trainable. 'False' are frozen.\")\n",
        "\n",
        "print(\"=============\")\n",
        "print(named_model, \":\")\n",
        "print(\"=============\")\n",
        "print(model)\n",
        "print(\"=============\")\n",
        "print(model.config)\n",
        "print(\"=============\")\n",
        "print(\"num_parameters:\", model.num_parameters())\n",
        "print(\"=============\")\n",
        "print(\"num_trainable_parameters:\", model.num_parameters(only_trainable=True))"
      ],
      "metadata": {
        "id": "nFVChQwSsJp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.1 Results"
      ],
      "metadata": {
        "id": "cs7O-34q2uQR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train & Evaluate\n",
        "\n",
        "trained_model, trainer_obj = train_transformer_model(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_data_hf,\n",
        "    val_dataset=val_data_hf,\n",
        "    output_dir=dir_results,\n",
        "    num_epochs=num_epochs,\n",
        "    batch_size=size_batch,\n",
        "    lr=learning_rate,\n",
        "    weight_decay=regularization_weight_decay\n",
        ")\n",
        "\n",
        "metrics = trainer_obj.evaluate()\n",
        "print(\"Validation metrics:\", metrics)\n",
        "\n",
        "test_metrics = trainer_obj.evaluate(test_data_hf) if test_data_hf else None\n",
        "print(\"Test metrics:\", test_metrics)"
      ],
      "metadata": {
        "id": "h3pwvxuesJo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Experiment configuration used with this experiment:\")\n",
        "print(\"model used:\", named_model)\n",
        "print(\"learning rate used:\", learning_rate)\n",
        "print(\"number of epochs:\", num_epochs)\n",
        "print(\"maximum sequence length:\", length_max)\n",
        "print(\"batch size used:\", size_batch)\n",
        "print(\"regularization value:\", regularization_weight_decay)\n",
        "print(\"outcome variable:\", y_col)\n",
        "print(\"task:\", x_task)\n",
        "print(\"input column:\", x_col)"
      ],
      "metadata": {
        "id": "aysFT4tAsJoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save model checkpoint\n",
        "\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "model_save_path = os.path.join(dir_models, f\"{x_task}_{named_model}_{y_col}_{timestamp}\")\n",
        "\n",
        "trainer_obj.save_model(model_save_path)\n",
        "print(f\"Model checkpoint saved to: {model_save_path}\")"
      ],
      "metadata": {
        "id": "ZA_bWdNSy27t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experiment 1.2: from pre-trained bert-base-cased multi task 2"
      ],
      "metadata": {
        "id": "214_LGGAzEWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Experiment Parameters\n",
        "\n",
        "named_model = \"bert-base-cased\"\n",
        "# named_model = \"roberta-base\"\n",
        "# named_model = \"bert-large\"\n",
        "# named_model = \"roberta-large\"\n",
        "# named_model = \"\" # modern bert\n",
        "\n",
        "# learning_rate = 1e-3\n",
        "# learning_rate = 1e-4\n",
        "# learning_rate = 1e-5\n",
        "# learning_rate = 5e-6\n",
        "# learning_rate = 5e-7\n",
        "learning_rate = 1e-8\n",
        "\n",
        "# num_epochs = 3\n",
        "num_epochs = 5\n",
        "# num_epochs = 10\n",
        "# num_epochs = 15\n",
        "# num_epochs = 20\n",
        "\n",
        "length_max = 128\n",
        "# length_max = 256\n",
        "# length_max = 348\n",
        "# length_max = 512\n",
        "\n",
        "# size_batch = 1\n",
        "size_batch = 4\n",
        "# size_batch = 8\n",
        "# size_batch = 16\n",
        "# size_batch = 24\n",
        "# size_batch = 32\n",
        "\n",
        "# regularization_weight_decay = 0\n",
        "regularization_weight_decay = 0.1\n",
        "# regularization_weight_decay = 0.5\n",
        "\n",
        "y_col = \"binary_complexity\"\n",
        "# y_col = \"complexity\"\n",
        "\n",
        "# x_task = \"single\"\n",
        "x_task = \"multi\"\n",
        "\n",
        "# x_col = \"sentence\"\n",
        "x_col = \"sentence_no_contractions\"\n",
        "# x_col = \"pos_sequence\"\n",
        "# x_col = \"dep_sequence\"\n",
        "# x_col = \"morph_sequence\"\n",
        "\n",
        "if x_task == \"single\":\n",
        "    df_train = train_single_df\n",
        "    df_val   = trial_val_single_df\n",
        "    df_test  = test_single_df\n",
        "else:\n",
        "    df_train = train_multi_df\n",
        "    df_val   = trial_val_multi_df\n",
        "    df_test  = test_multi_df"
      ],
      "metadata": {
        "id": "_tqrSUw_zYLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"model checkpoints:\", dir_models)\n",
        "!ls /content/drive/MyDrive/266-final/models/"
      ],
      "metadata": {
        "id": "UtkPOzC0zYK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Model & Tokenizer\n",
        "model, tokenizer = get_model_and_tokenizer(named_model) # deprecated argument structure\n",
        "# model, tokenizer = get_model_and_tokenizer(\"/content/drive/MyDrive/266-final/models/bert-base-cased_20250407_232900\") # proposed argument usage for checkpointed models\n",
        "\n",
        "# for name, param in model.named_parameters():\n",
        "#     print(name)\n",
        "\n",
        "print(\"=============\")\n",
        "print(named_model, \":\")\n",
        "print(\"=============\")\n",
        "# print(model)\n",
        "print(\"=============\")\n",
        "print(model.config)\n",
        "# print(\"=============\")\n",
        "# print(\"num_parameters:\", model.num_parameters())\n",
        "# print(\"=============\")\n",
        "# print(\"num_trainable_parameters:\", model.num_parameters(only_trainable=True))"
      ],
      "metadata": {
        "id": "ApJjvZKmzYKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze/Unfreeze Layers & Additional Configuration Parameters\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "layers_to_unfreeze = [\n",
        "    \"bert.embeddings.\",\n",
        "    \"bert.encoder.layer.0.\",\n",
        "    \"bert.encoder.layer.1.\",\n",
        "    \"bert.encoder.layer.9.\",\n",
        "    \"bert.encoder.layer.9.\",\n",
        "    \"bert.encoder.layer.10.\",\n",
        "    \"bert.encoder.layer.11.\",\n",
        "    \"bert.pooler.\",\n",
        "    \"classifier.\",\n",
        "]\n",
        "\n",
        "freeze_unfreeze_layers(model, layers_to_unfreeze=layers_to_unfreeze)\n",
        "\n",
        "\n",
        "bert_config = BertConfig(\n",
        "    # vocab_size=28996,\n",
        "    hidden_size=768,\n",
        "    # num_hidden_layers=12,\n",
        "    # num_attention_heads=12,\n",
        "    intermediate_size=6144,\n",
        "    # max_position_embeddings=512,\n",
        "    type_vocab_size=2,\n",
        "\n",
        "    hidden_dropout_prob=0.1,\n",
        "    attention_probs_dropout_prob=0.1,\n",
        "    # classifier_dropout=None,\n",
        "    # initializer_range=0.02,\n",
        "    # layer_norm_eps=1e-12,\n",
        "\n",
        "    hidden_act=\"gelu\",\n",
        "    gradient_checkpointing=True,\n",
        "    position_embedding_type=\"absolute\",\n",
        "    use_cache=True,\n",
        "    pad_token_id=0\n",
        ")\n",
        "\n",
        "model.bert.pooler.activation = nn.ReLU() # Tanh() replaced as the pooler layer activation function\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    print(name, \"requires_grad=\", param.requires_grad)\n",
        "\n",
        "print(\"\\nLayers that are 'True' are trainable. 'False' are frozen.\")\n",
        "\n",
        "print(\"=============\")\n",
        "print(named_model, \":\")\n",
        "print(\"=============\")\n",
        "print(model)\n",
        "print(\"=============\")\n",
        "print(model.config)\n",
        "print(\"=============\")\n",
        "print(\"num_parameters:\", model.num_parameters())\n",
        "print(\"=============\")\n",
        "print(\"num_trainable_parameters:\", model.num_parameters(only_trainable=True))"
      ],
      "metadata": {
        "id": "tX01cTvJzYJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2 Results"
      ],
      "metadata": {
        "id": "I6SqiL4j2xkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train & Evaluate\n",
        "\n",
        "trained_model, trainer_obj = train_transformer_model(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_data_hf,\n",
        "    val_dataset=val_data_hf,\n",
        "    output_dir=dir_results,\n",
        "    num_epochs=num_epochs,\n",
        "    batch_size=size_batch,\n",
        "    lr=learning_rate,\n",
        "    weight_decay=regularization_weight_decay\n",
        ")\n",
        "\n",
        "metrics = trainer_obj.evaluate()\n",
        "print(\"Validation metrics:\", metrics)\n",
        "\n",
        "test_metrics = trainer_obj.evaluate(test_data_hf) if test_data_hf else None\n",
        "print(\"Test metrics:\", test_metrics)"
      ],
      "metadata": {
        "id": "5a08UyPTzXxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Experiment configuration used with this experiment:\")\n",
        "print(\"model used:\", named_model)\n",
        "print(\"learning rate used:\", learning_rate)\n",
        "print(\"number of epochs:\", num_epochs)\n",
        "print(\"maximum sequence length:\", length_max)\n",
        "print(\"batch size used:\", size_batch)\n",
        "print(\"regularization value:\", regularization_weight_decay)\n",
        "print(\"outcome variable:\", y_col)\n",
        "print(\"task:\", x_task)\n",
        "print(\"input column:\", x_col)"
      ],
      "metadata": {
        "id": "K7c_CaznzqcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save model checkpoint\n",
        "\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "model_save_path = os.path.join(dir_models, f\"{x_task}_{named_model}_{y_col}_{timestamp}\")\n",
        "\n",
        "trainer_obj.save_model(model_save_path)\n",
        "print(f\"Model checkpoint saved to: {model_save_path}\")"
      ],
      "metadata": {
        "id": "YralHU70zqbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experiment 1.3: from checkpoint 1.0 or 1.1 bert-base-cased MSFT'd single -> multi"
      ],
      "metadata": {
        "id": "H5PUNtHezMCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Experiment Parameters\n",
        "\n",
        "named_model = \"bert-base-cased\"\n",
        "# named_model = \"roberta-base\"\n",
        "# named_model = \"bert-large\"\n",
        "# named_model = \"roberta-large\"\n",
        "# named_model = \"\" # modern bert\n",
        "\n",
        "# learning_rate = 1e-3\n",
        "# learning_rate = 1e-4\n",
        "# learning_rate = 1e-5\n",
        "# learning_rate = 5e-6\n",
        "# learning_rate = 5e-7\n",
        "learning_rate = 1e-8\n",
        "\n",
        "# num_epochs = 3\n",
        "num_epochs = 5\n",
        "# num_epochs = 10\n",
        "# num_epochs = 15\n",
        "# num_epochs = 20\n",
        "\n",
        "length_max = 128\n",
        "# length_max = 256\n",
        "# length_max = 348\n",
        "# length_max = 512\n",
        "\n",
        "# size_batch = 1\n",
        "size_batch = 4\n",
        "# size_batch = 8\n",
        "# size_batch = 16\n",
        "# size_batch = 24\n",
        "# size_batch = 32\n",
        "\n",
        "# regularization_weight_decay = 0\n",
        "regularization_weight_decay = 0.1\n",
        "# regularization_weight_decay = 0.5\n",
        "\n",
        "y_col = \"binary_complexity\"\n",
        "# y_col = \"complexity\"\n",
        "\n",
        "# x_task = \"single\"\n",
        "x_task = \"multi\"\n",
        "\n",
        "# x_col = \"sentence\"\n",
        "x_col = \"sentence_no_contractions\"\n",
        "# x_col = \"pos_sequence\"\n",
        "# x_col = \"dep_sequence\"\n",
        "# x_col = \"morph_sequence\"\n",
        "\n",
        "if x_task == \"single\":\n",
        "    df_train = train_single_df\n",
        "    df_val   = trial_val_single_df\n",
        "    df_test  = test_single_df\n",
        "else:\n",
        "    df_train = train_multi_df\n",
        "    df_val   = trial_val_multi_df\n",
        "    df_test  = test_multi_df"
      ],
      "metadata": {
        "id": "cUkQp35ezzO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"model checkpoints:\", dir_models)\n",
        "!ls /content/drive/MyDrive/266-final/models/"
      ],
      "metadata": {
        "id": "N8VuzNa0zzOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Model & Tokenizer\n",
        "# model, tokenizer = get_model_and_tokenizer(named_model) # deprecated argument structure\n",
        "model, tokenizer = get_model_and_tokenizer(\"/content/drive/MyDrive/266-final/models/....\") # proposed argument usage for checkpointed models\n",
        "\n",
        "# for name, param in model.named_parameters():\n",
        "#     print(name)\n",
        "\n",
        "print(\"=============\")\n",
        "print(named_model, \":\")\n",
        "print(\"=============\")\n",
        "# print(model)\n",
        "print(\"=============\")\n",
        "print(model.config)\n",
        "# print(\"=============\")\n",
        "# print(\"num_parameters:\", model.num_parameters())\n",
        "# print(\"=============\")\n",
        "# print(\"num_trainable_parameters:\", model.num_parameters(only_trainable=True))"
      ],
      "metadata": {
        "id": "fkKVx2jkzzNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze/Unfreeze Layers & Additional Configuration Parameters\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "layers_to_unfreeze = [\n",
        "    \"bert.embeddings.\",\n",
        "    \"bert.encoder.layer.0.\",\n",
        "    \"bert.encoder.layer.1.\",\n",
        "    \"bert.encoder.layer.9.\",\n",
        "    \"bert.encoder.layer.9.\",\n",
        "    \"bert.encoder.layer.10.\",\n",
        "    \"bert.encoder.layer.11.\",\n",
        "    \"bert.pooler.\",\n",
        "    \"classifier.\",\n",
        "]\n",
        "\n",
        "freeze_unfreeze_layers(model, layers_to_unfreeze=layers_to_unfreeze)\n",
        "\n",
        "\n",
        "bert_config = BertConfig(\n",
        "    # vocab_size=28996,\n",
        "    hidden_size=768,\n",
        "    # num_hidden_layers=12,\n",
        "    # num_attention_heads=12,\n",
        "    intermediate_size=6144,\n",
        "    # max_position_embeddings=512,\n",
        "    type_vocab_size=2,\n",
        "\n",
        "    hidden_dropout_prob=0.1,\n",
        "    attention_probs_dropout_prob=0.1,\n",
        "    # classifier_dropout=None,\n",
        "    # initializer_range=0.02,\n",
        "    # layer_norm_eps=1e-12,\n",
        "\n",
        "    hidden_act=\"gelu\",\n",
        "    gradient_checkpointing=True,\n",
        "    position_embedding_type=\"absolute\",\n",
        "    use_cache=True,\n",
        "    pad_token_id=0\n",
        ")\n",
        "\n",
        "model.bert.pooler.activation = nn.ReLU() # Tanh() replaced as the pooler layer activation function\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    print(name, \"requires_grad=\", param.requires_grad)\n",
        "\n",
        "print(\"\\nLayers that are 'True' are trainable. 'False' are frozen.\")\n",
        "\n",
        "print(\"=============\")\n",
        "print(named_model, \":\")\n",
        "print(\"=============\")\n",
        "print(model)\n",
        "print(\"=============\")\n",
        "print(model.config)\n",
        "print(\"=============\")\n",
        "print(\"num_parameters:\", model.num_parameters())\n",
        "print(\"=============\")\n",
        "print(\"num_trainable_parameters:\", model.num_parameters(only_trainable=True))"
      ],
      "metadata": {
        "id": "jE9w2ot3zy3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.3 Results"
      ],
      "metadata": {
        "id": "qJfC13bC20XX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train & Evaluate\n",
        "\n",
        "trained_model, trainer_obj = train_transformer_model(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_data_hf,\n",
        "    val_dataset=val_data_hf,\n",
        "    output_dir=dir_results,\n",
        "    num_epochs=num_epochs,\n",
        "    batch_size=size_batch,\n",
        "    lr=learning_rate,\n",
        "    weight_decay=regularization_weight_decay\n",
        ")\n",
        "\n",
        "metrics = trainer_obj.evaluate()\n",
        "print(\"Validation metrics:\", metrics)\n",
        "\n",
        "test_metrics = trainer_obj.evaluate(test_data_hf) if test_data_hf else None\n",
        "print(\"Test metrics:\", test_metrics)"
      ],
      "metadata": {
        "id": "mkVts0GzzyzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Experiment configuration used with this experiment:\")\n",
        "print(\"model used:\", named_model)\n",
        "print(\"learning rate used:\", learning_rate)\n",
        "print(\"number of epochs:\", num_epochs)\n",
        "print(\"maximum sequence length:\", length_max)\n",
        "print(\"batch size used:\", size_batch)\n",
        "print(\"regularization value:\", regularization_weight_decay)\n",
        "print(\"outcome variable:\", y_col)\n",
        "print(\"task:\", x_task)\n",
        "print(\"input column:\", x_col)"
      ],
      "metadata": {
        "id": "k2tOUzMCzSmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save model checkpoint\n",
        "\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "model_save_path = os.path.join(dir_models, f\"{x_task}_{named_model}_{y_col}_{timestamp}\")\n",
        "\n",
        "trainer_obj.save_model(model_save_path)\n",
        "print(f\"Model checkpoint saved to: {model_save_path}\")"
      ],
      "metadata": {
        "id": "ldub-JG30PQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gj2j182Q0PI1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}